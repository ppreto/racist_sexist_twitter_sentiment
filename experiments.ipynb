{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T09:55:22.846403Z",
     "start_time": "2018-12-10T09:55:21.724633Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T09:55:23.022274Z",
     "start_time": "2018-12-10T09:55:22.848409Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NORMALIZATION AND CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### counting hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:13:01.986427Z",
     "start_time": "2018-12-02T23:13:01.922225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def separate_hashtags(tweet):\n",
    "    tweet = re.sub(r'#', ' #', tweet)\n",
    "    return tweet\n",
    "df['tweet_sep_hasht'] = df.tweet.apply(separate_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:13:05.435106Z",
     "start_time": "2018-12-02T23:13:04.904605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hashtag_list_normal = []\n",
    "hashtag_list_racist = []\n",
    "i = 0\n",
    "for t in df['tweet_sep_hasht']:\n",
    "    if df.label.iloc[i] == 0:\n",
    "        hashtag_list_normal += [w.strip(\"#\") for w in t.split() if w.startswith(\"#\")]\n",
    "    else:\n",
    "        hashtag_list_racist += [w.strip(\"#\") for w in t.split() if w.startswith(\"#\")]\n",
    "    i+=1\n",
    "    \n",
    "counts_n = Counter(hashtag_list_normal)\n",
    "counts_r = Counter(hashtag_list_racist)\n",
    "\n",
    "list50_best = [tup[0] for tup in sorted(counts_n.items(), key=lambda x: x[1])[-50:] if len(tup[0])>1]\n",
    "list50_worst = [tup[0] for tup in sorted(counts_r.items(), key=lambda x: x[1])[-50:] if len(tup[0])>1]\n",
    "list100_best_worst = list50_best + list50_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:13:08.914856Z",
     "start_time": "2018-12-02T23:13:08.678087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "relevant_hastags_matrix = np.zeros([df.shape[0], 100])\n",
    "i = 0\n",
    "for t in df['tweet_sep_hasht']:\n",
    "    hashtag_list = [w.strip(\"#\") for w in t.split() if w.startswith(\"#\")]\n",
    "    for h in hashtag_list:\n",
    "        try:\n",
    "            ind = list100_best_worst.index(h)\n",
    "            relevant_hastags_matrix[i,ind] = 1\n",
    "        except:\n",
    "            pass\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### cleaning and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T09:58:49.966730Z",
     "start_time": "2018-12-10T09:56:31.528299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textacy.preprocess import preprocess_text\n",
    "df['tweet_processed'] = df['tweet'].apply(lambda x: preprocess_text(x, fix_unicode=True, lowercase=True, no_urls=True, \n",
    "                no_emails=True, no_phone_numbers=True, no_numbers=True, \n",
    "                no_currency_symbols=True, no_punct=True, no_accents=True, no_contractions=True))\n",
    "df.tweet_processed = df.tweet_processed.apply(lambda x: separate_emojis(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T09:55:41.004816Z",
     "start_time": "2018-12-10T09:55:40.979748Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "keys = list(emoji.EMOJI_UNICODE.values())\n",
    "values = [\" \"+i+\" \"for i in keys]\n",
    "\n",
    "del values[2620]\n",
    "del keys[2620]\n",
    "dictionary_emojis = dict(zip(keys, values))\n",
    "\n",
    "import re\n",
    "\n",
    "def separate_emojis(text):\n",
    "    # use these three lines to do the replacement\n",
    "    rep = dict((re.escape(k), v) for k, v in dictionary_emojis.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:23:31.496812Z",
     "start_time": "2018-12-03T21:23:31.462722Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model i love u take with u all the time in urðŸ“± ðŸ˜™ðŸ˜ŽðŸ‘„ðŸ‘…ðŸ’¦ðŸ’¦ðŸ’¦'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet_processed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:35.902970Z",
     "start_time": "2018-12-03T21:23:31.497815Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.tweet_processed = df.tweet_processed.apply(lambda x: separate_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:35.950009Z",
     "start_time": "2018-12-03T21:25:35.904918Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model i love u take with u all the time in ur ðŸ“±   ðŸ˜™  ðŸ˜Ž  ðŸ‘„  ðŸ‘…  ðŸ’¦  ðŸ’¦  ðŸ’¦ '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet_processed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:36.145626Z",
     "start_time": "2018-12-03T21:25:35.951013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove double+ whitespace and words with only one letter\n",
    "def processTweet(tweet):\n",
    "    #-- Remove words with 1 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1}\\b', '', tweet)\n",
    "    #-- Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    #-- Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ')\n",
    "    return tweet\n",
    "\n",
    "df.tweet_processed = df.tweet_processed.apply(lambda x: processTweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### REMOVE STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:36.156564Z",
     "start_time": "2018-12-03T21:25:36.147540Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#df['tweet_processed'] = df['tweet_processed'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:42.113649Z",
     "start_time": "2018-12-03T21:25:36.159572Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['tweet_processed'] = df['tweet_processed'].apply(lambda x: \" \".join([porter_stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:42.118576Z",
     "start_time": "2018-12-03T21:25:42.115568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:42.558716Z",
     "start_time": "2018-12-03T21:25:42.120581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(max_features=4000, ngram_range=(1, 1))\n",
    "tf_idf_vector = tvec.fit_transform(df.tweet_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:17:43.942272Z",
     "start_time": "2018-12-02T23:17:43.352067Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:17:44.773382Z",
     "start_time": "2018-12-02T23:17:44.542800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = []\n",
    "for i,t in zip(df.index,df.tweet_processed):\n",
    "    labeled_tweets.append(LabeledSentence(t.split(), [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:17:52.076788Z",
     "start_time": "2018-12-02T23:17:52.042698Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0,size=100,negative=5,min_count=2,alpha=0.065,min_alpha=0.065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:18:24.417687Z",
     "start_time": "2018-12-02T23:17:54.183396Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dbow.build_vocab([x for x in labeled_tweets])\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in labeled_tweets]), total_examples=len(labeled_tweets), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:18:24.543021Z",
     "start_time": "2018-12-02T23:18:24.431751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc2_vecs = np.zeros((df.shape[0], 100))\n",
    "j = 0\n",
    "for i in df.index:\n",
    "    prefix = str(i)\n",
    "    doc2_vecs[i] = model_dbow.docvecs[i]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding (glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T10:01:28.895601Z",
     "start_time": "2018-12-10T10:01:27.093722Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "glove_twitter = api.load(\"glove-twitter-200\")\n",
    "\n",
    "embed_size = 200 # size of each word vector\n",
    "max_features = 4000 # amount of unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a comment to use\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df.tweet_processed.tolist())\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df.tweet_processed.tolist())\n",
    "X_w2v = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create a document representation using the embeddings of each word\n",
    "X_glove = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'sum') for z in df.tweet_processed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:31:37.381485Z",
     "start_time": "2018-12-02T22:31:35.491788Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:31:35.478751Z",
     "start_time": "2018-12-02T22:31:35.462709Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embed_size = 200 # how big is each word vector\n",
    "max_features = 4000 # how many unique words to use (i.e num rows in embedding vector) used 20000 beacause in the training that value contains all the tokens\n",
    "maxlen = 50 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:32:50.556892Z",
     "start_time": "2018-12-02T22:32:49.705772Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df.tweet_processed.tolist())\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df.tweet_processed.tolist())\n",
    "X_w2v = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:32:52.244580Z",
     "start_time": "2018-12-02T22:32:51.174735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#copute mean and std to fill words that were not trained\n",
    "emb_mean,emb_std = glove_twitter.vectors.mean(), glove_twitter.vectors.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:32:52.294713Z",
     "start_time": "2018-12-02T22:32:52.245584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = glove_twitter[word]\n",
    "    except:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Joining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:53.013925Z",
     "start_time": "2018-12-03T21:25:52.673979Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#X = np.concatenate([tf_idf_vector.toarray(), doc2_vecs, relevant_hastags_matrix], axis=1)\n",
    "X = tf_idf_vector.toarray()\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:25:48.714301Z",
     "start_time": "2018-12-03T21:25:42.775295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X = scale(X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:22:31.478627Z",
     "start_time": "2018-12-02T23:22:30.848983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T08:37:37.368823Z",
     "start_time": "2018-12-03T08:35:15.086471Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-135b53c756ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'arpack'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'randomized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;31m# Get variance explained by singular values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mtotal_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m   3192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[1;32m-> 3194\u001b[1;33m                          **kwargs)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Note that x may not be inexact and that we need it to be an array,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# not a scalar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=750)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:49:42.887682Z",
     "start_time": "2018-12-02T20:47:17.727227Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\incremental_pca.py:294: RuntimeWarning: Mean of empty slice.\n",
      "  explained_variance[self.n_components_:].mean()\n",
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "ipca = IncrementalPCA(n_components=150, batch_size=150)\n",
    "X = ipca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:37:15.419597Z",
     "start_time": "2018-12-02T23:37:14.837670Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:30:37.437178Z",
     "start_time": "2018-12-03T21:30:37.384900Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve, classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:30:39.229232Z",
     "start_time": "2018-12-03T21:30:39.205137Z"
    }
   },
   "outputs": [],
   "source": [
    "base_weights = 1/(df.label.value_counts()/max(df.label.value_counts())).values\n",
    "weights = np.array([base_weights[0]  if x == 0 else base_weights[1] for x in df.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:30:40.624838Z",
     "start_time": "2018-12-03T21:30:40.621830Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T21:39:24.993443Z",
     "start_time": "2018-12-03T21:35:36.900173Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "creating model\n",
      "predicting\n",
      "Train report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23776\n",
      "           1       1.00      1.00      1.00      1793\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     25569\n",
      "   macro avg       1.00      1.00      1.00     25569\n",
      "weighted avg       1.00      1.00      1.00     25569\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5944\n",
      "           1       0.91      0.40      0.56       449\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      6393\n",
      "   macro avg       0.94      0.70      0.77      6393\n",
      "weighted avg       0.95      0.96      0.95      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create object for cross validation (5 folds)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "fold = 1\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "print(\"training\")\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    weights_train = weights[train_index]\n",
    "    \n",
    "    \"\"\"\n",
    "    #sample data\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    d = dict(zip(unique, counts))\n",
    "    # undersample data points\n",
    "    \n",
    "    undersampler = RandomUnderSampler(sampling_strategy={0: d[1],\n",
    "                                                         1: d[1]})\n",
    "    \n",
    "    oversampler = SMOTE(sampling_strategy={0: d[0],\n",
    "                                           1: d[0],\n",
    "                                           },\n",
    "                        random_state=20)\n",
    "    \"\"\"\n",
    "    #X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "    #X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    #X_resampled[X_resampled==-1] = np.NaN\n",
    "    \n",
    "    #X_resampled = pd.DataFrame(X_resampled, columns=X_train.columns.values)\n",
    "    print(\"creating model\")\n",
    "    xgb_model = RandomForestClassifier(n_estimators=500, class_weight='balanced', n_jobs=-1).fit(X_train, y_train)\n",
    "    \"\"\"xgb_model = XGBClassifier(objective='binary:logistic',\n",
    "                              n_estimators=20,\n",
    "                              max_depth=8,\n",
    "                              learning_rate=0.025,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=20)\n",
    "    print(\"training model\")\n",
    "    xgb_model.fit(X_train, \n",
    "                  y_train,\n",
    "                  #sample_weight=weights_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  early_stopping_rounds=30,\n",
    "                  verbose=False\n",
    "                  )\n",
    "    \"\"\"\n",
    "    print(\"predicting\")\n",
    "    # predict outputs\n",
    "    train_predictions = xgb_model.predict(X_train)\n",
    "    train_actuals = y_train\n",
    "    \n",
    "    predictions = xgb_model.predict(X_test)\n",
    "    prediction_probs = xgb_model.predict_proba(X_test)\n",
    "    actuals = y_test\n",
    "    \n",
    "    ## get train performance metrics\n",
    "    #print(\"Fold: \", fold)\n",
    "    print('Train report')\n",
    "    print(classification_report(train_actuals, train_predictions)) \n",
    "    #print('------------')\n",
    "    \n",
    "    # get performance metrics\n",
    "    print('Test report')\n",
    "    print(classification_report(actuals, predictions)) \n",
    "    #precisions.append(precision_score(actuals, predictions, average=None))\n",
    "    #recalls.append(recall_score(actuals, predictions, average=None))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T16:49:08.517210Z",
     "start_time": "2018-11-30T16:49:08.501630Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential #Sequential Models\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Dropout #Dense Fully Connected Layer Type\n",
    "from keras.optimizers import SGD #Stochastic Gradient Descent Optimizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T18:58:27.792910Z",
     "start_time": "2018-11-30T18:58:27.771854Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_network(n_columns, lr=0.001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_shape=(n_columns,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "              \n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "    #stochastic gradient descent\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T18:58:29.834512Z",
     "start_time": "2018-11-30T18:58:29.829498Z"
    }
   },
   "outputs": [],
   "source": [
    "base_weights = 1/(df.label.value_counts()/max(df.label.value_counts())).values\n",
    "#base_weights[1] = base_weights[1]*1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T19:06:04.850678Z",
     "start_time": "2018-11-30T18:58:38.247218Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 297us/step - loss: 1.7911 - val_loss: 2.4566\n",
      "0.18470539072294187\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 1.9389 - val_loss: 0.5486\n",
      "0.36003861003861\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 206us/step - loss: 0.7930 - val_loss: 0.2035\n",
      "0.4963350785340314\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.7822 - val_loss: 0.2953\n",
      "0.4662402274342572\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.6225 - val_loss: 0.2720\n",
      "0.4816446402349487\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.5653 - val_loss: 0.2522\n",
      "0.4928193499622071\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.5140 - val_loss: 0.2265\n",
      "0.5181598062953995\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.4728 - val_loss: 0.2096\n",
      "0.525781910397295\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 210us/step - loss: 0.4389 - val_loss: 0.2014\n",
      "0.5242718446601942\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 204us/step - loss: 0.4089 - val_loss: 0.2001\n",
      "0.530396475770925\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 214us/step - loss: 0.3807 - val_loss: 0.2039\n",
      "0.5340136054421769\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 199us/step - loss: 0.3536 - val_loss: 0.2078\n",
      "0.5353535353535354\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 195us/step - loss: 0.3284 - val_loss: 0.2095\n",
      "0.5312239800166528\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 204us/step - loss: 0.3051 - val_loss: 0.2059\n",
      "0.5392491467576791\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.2835 - val_loss: 0.1982\n",
      "0.5489849955869374\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 196us/step - loss: 0.2628 - val_loss: 0.1881\n",
      "0.5562790697674419\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 193us/step - loss: 0.2430 - val_loss: 0.1784\n",
      "0.56640625\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.2241 - val_loss: 0.1704\n",
      "0.5740551583248212\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.2063 - val_loss: 0.1651\n",
      "0.5811965811965812\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 195us/step - loss: 0.1896 - val_loss: 0.1619\n",
      "0.588495575221239\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 197us/step - loss: 0.1741 - val_loss: 0.1603\n",
      "0.5963718820861678\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 198us/step - loss: 0.1598 - val_loss: 0.1594\n",
      "0.5931034482758621\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.1464 - val_loss: 0.1592\n",
      "0.6000000000000001\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.1341 - val_loss: 0.1591\n",
      "0.6072684642438453\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.1227 - val_loss: 0.1593\n",
      "0.6137689614935823\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.1122 - val_loss: 0.1596\n",
      "0.6223776223776224\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.1025 - val_loss: 0.1602\n",
      "0.6248548199767712\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0938 - val_loss: 0.1609\n",
      "0.6242774566473988\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0858 - val_loss: 0.1619\n",
      "0.6265895953757226\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0785 - val_loss: 0.1630\n",
      "0.621559633027523\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0719 - val_loss: 0.1642\n",
      "0.6214039125431532\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0658 - val_loss: 0.1655\n",
      "0.6171428571428571\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0603 - val_loss: 0.1669\n",
      "0.6197502837684449\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0552 - val_loss: 0.1683\n",
      "0.6162528216704288\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0506 - val_loss: 0.1698\n",
      "0.6164229471316085\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.0464 - val_loss: 0.1712\n",
      "0.6122905027932961\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 193us/step - loss: 0.0426 - val_loss: 0.1727\n",
      "0.6112956810631228\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0391 - val_loss: 0.1743\n",
      "0.6106194690265487\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 198us/step - loss: 0.0359 - val_loss: 0.1759\n",
      "0.6108048511576627\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 231us/step - loss: 0.0330 - val_loss: 0.1776\n",
      "0.6086956521739131\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0303 - val_loss: 0.1795\n",
      "0.6040992448759439\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0279 - val_loss: 0.1813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-b78d95192a66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                   validation_data=(x_test, y_test))\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mf1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1025\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[1;32m-> 1027\u001b[1;33m                                   steps=steps)\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1798\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1800\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1299\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "f = []\n",
    "#start cross validation\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "    #create model\n",
    "    model = create_network(x_train.shape[1], 0.05)\n",
    "    for i in range(100):\n",
    "        model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n",
    "                      epochs=1, verbose=1, class_weight={0:base_weights[0],1:base_weights[1]}, \n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        y_train_pred = model.predict(x_train).argmax(axis=1)\n",
    "        y_test_pred = model.predict(x_test).argmax(axis=1)\n",
    "        f1s = metrics.f1_score(y_test, y_test_pred)\n",
    "        f.append(f1s)\n",
    "        print(f1s)\n",
    "        #print(\"train\\n\",metrics.classification_report(y_train, y_train_pred))\n",
    "        #print(\"test\\n\",metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T19:07:36.022674Z",
     "start_time": "2018-11-30T19:07:35.980520Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_network(n_columns, lr=0.001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_shape=(n_columns,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "              \n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    #stochastic gradient descent\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T20:36:20.491331Z",
     "start_time": "2018-11-30T19:11:59.023557Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 311us/step - loss: 1.5358 - val_loss: 0.8981\n",
      "0.21091997008227376\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 183us/step - loss: 0.8940 - val_loss: 0.4814\n",
      "0.3337868480725624\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 184us/step - loss: 0.7222 - val_loss: 0.4165\n",
      "0.3738317757009346\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.6603 - val_loss: 0.3833\n",
      "0.4011299435028248\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.6212 - val_loss: 0.3484\n",
      "0.4297820823244553\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.5882 - val_loss: 0.3115\n",
      "0.4441575209812783\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 184us/step - loss: 0.5555 - val_loss: 0.2750\n",
      "0.47491166077738517\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.5222 - val_loss: 0.2432\n",
      "0.5022970903522205\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 11s 413us/step - loss: 0.4894 - val_loss: 0.2179\n",
      "0.529074529074529\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 292us/step - loss: 0.4581 - val_loss: 0.1992\n",
      "0.5512367491166078\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 250us/step - loss: 0.4294 - val_loss: 0.1864\n",
      "0.5583566760037348\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 262us/step - loss: 0.4034 - val_loss: 0.1779\n",
      "0.5700483091787439\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.3799 - val_loss: 0.1726\n",
      "0.5801376597836776\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.3582 - val_loss: 0.1695\n",
      "0.5810945273631841\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 269us/step - loss: 0.3378 - val_loss: 0.1681\n",
      "0.5868263473053891\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 253us/step - loss: 0.3181 - val_loss: 0.1678\n",
      "0.586\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.2990 - val_loss: 0.1677\n",
      "0.5890547263681591\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.2805 - val_loss: 0.1672\n",
      "0.5902293120638086\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.2626 - val_loss: 0.1660\n",
      "0.5965965965965967\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.2455 - val_loss: 0.1641\n",
      "0.5925176946410515\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.2292 - val_loss: 0.1614\n",
      "0.6018423746161718\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 269us/step - loss: 0.2138 - val_loss: 0.1586\n",
      "0.6075156576200418\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.1993 - val_loss: 0.1560\n",
      "0.6102783725910065\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.1856 - val_loss: 0.1539\n",
      "0.6111719605695509\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.1729 - val_loss: 0.1524\n",
      "0.6140155728587319\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 257us/step - loss: 0.1609 - val_loss: 0.1514\n",
      "0.6133032694475762\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.1497 - val_loss: 0.1509\n",
      "0.6068181818181818\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.1392 - val_loss: 0.1506\n",
      "0.611683848797251\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.1295 - val_loss: 0.1505\n",
      "0.6102857142857143\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.1204 - val_loss: 0.1507\n",
      "0.6111744583808437\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.1119 - val_loss: 0.1510\n",
      "0.6136363636363636\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.1041 - val_loss: 0.1514\n",
      "0.6092865232163079\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.0969 - val_loss: 0.1520\n",
      "0.6098654708520179\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.0902 - val_loss: 0.1527\n",
      "0.6119733924611973\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 258us/step - loss: 0.0839 - val_loss: 0.1536\n",
      "0.614709110867179\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0782 - val_loss: 0.1546\n",
      "0.6132177681473456\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 256us/step - loss: 0.0728 - val_loss: 0.1556\n",
      "0.6167023554603854\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0679 - val_loss: 0.1567\n",
      "0.6178343949044586\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0633 - val_loss: 0.1579\n",
      "0.6158730158730158\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 259us/step - loss: 0.0591 - val_loss: 0.1591\n",
      "0.6140904311251315\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0551 - val_loss: 0.1603\n",
      "0.6157068062827225\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.0515 - val_loss: 0.1615\n",
      "0.6153846153846154\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 264us/step - loss: 0.0481 - val_loss: 0.1628\n",
      "0.6149068322981367\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.0450 - val_loss: 0.1640\n",
      "0.6127049180327869\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0421 - val_loss: 0.1653\n",
      "0.6095820591233436\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0395 - val_loss: 0.1666\n",
      "0.6085192697768763\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.0370 - val_loss: 0.1680\n",
      "0.6066734074823055\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.0347 - val_loss: 0.1694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6088709677419355\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 256us/step - loss: 0.0326 - val_loss: 0.1708\n",
      "0.6132264529058116\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 225us/step - loss: 0.0306 - val_loss: 0.1722\n",
      "0.6127744510978045\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 3930s 154ms/step - loss: 0.0288 - val_loss: 0.1736\n",
      "0.6123260437375746\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 298us/step - loss: 0.0271 - val_loss: 0.1750\n",
      "0.6106719367588933\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.0256 - val_loss: 0.1762\n",
      "0.6094674556213018\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.0241 - val_loss: 0.1775\n",
      "0.6070726915520629\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 227us/step - loss: 0.0228 - val_loss: 0.1788\n",
      "0.603515625\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 221us/step - loss: 0.0215 - val_loss: 0.1800\n",
      "0.6064139941690962\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 243us/step - loss: 0.0203 - val_loss: 0.1811\n",
      "0.6077669902912622\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 254us/step - loss: 0.0193 - val_loss: 0.1823\n",
      "0.6060019361084221\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 261us/step - loss: 0.0183 - val_loss: 0.1833\n",
      "0.6060019361084221\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 254us/step - loss: 0.0173 - val_loss: 0.1843\n",
      "0.6051873198847262\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 248us/step - loss: 0.0165 - val_loss: 0.1852\n",
      "0.6063522617901829\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.0157 - val_loss: 0.1862\n",
      "0.6050096339113681\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 244us/step - loss: 0.0149 - val_loss: 0.1870\n",
      "0.6038461538461538\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 251us/step - loss: 0.0142 - val_loss: 0.1878\n",
      "0.6051873198847262\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 242us/step - loss: 0.0136 - val_loss: 0.1885\n",
      "0.6057692307692308\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 257us/step - loss: 0.0130 - val_loss: 0.1891\n",
      "0.6055930568948891\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 250us/step - loss: 0.0124 - val_loss: 0.1897\n",
      "0.6044273339749759\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 248us/step - loss: 0.0119 - val_loss: 0.1902\n",
      "0.6057692307692308\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.0114 - val_loss: 0.1906\n",
      "0.6065259117082534\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 255us/step - loss: 0.0110 - val_loss: 0.1911\n",
      "0.6040268456375838\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-4b07cf8460f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n\u001b[0;32m     13\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbase_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbase_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                   validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "f = []\n",
    "#start cross validation\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "    #create model\n",
    "    model = create_network(x_train.shape[1], 0.05)\n",
    "    for i in range(100):\n",
    "        model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n",
    "                      epochs=1, verbose=1, class_weight={0:base_weights[0],1:base_weights[1]}, \n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        y_train_pred = np.around(model.predict(x_train))\n",
    "        y_test_pred = np.around(model.predict(x_test))\n",
    "        f1s = metrics.f1_score(y_test, y_test_pred)\n",
    "        f.append(f1s)\n",
    "        print(f1s)\n",
    "        #print(\"train\\n\",metrics.classification_report(y_train, y_train_pred))\n",
    "        #print(\"test\\n\",metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:33:46.446520Z",
     "start_time": "2018-12-02T22:33:46.421455Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "#from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve, classification_report\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:33:47.782697Z",
     "start_time": "2018-12-02T22:33:47.770666Z"
    }
   },
   "outputs": [],
   "source": [
    "base_weights = 1/(df.label.value_counts()/max(df.label.value_counts())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:33:49.431432Z",
     "start_time": "2018-12-02T22:33:49.428397Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X_w2v\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T22:34:33.771Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "f_train = []\n",
    "f = []\n",
    "#start cross validation\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "    #create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length = maxlen, weights=[embedding_matrix],trainable=True))\n",
    "    model.add(Bidirectional(LSTM(100, dropout_U = 0.05, dropout_W = 0.05, return_sequences=False)))\n",
    "    #model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
    "    \n",
    "    \"\"\"inp = Input(shape=(maxlen,))#seq_length\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)#max_features-number of words in vocab #embed_size- size glove/word2vec vectors\n",
    "    x = Bidirectional(LSTM(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(2, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\"\"\"\n",
    "    \n",
    "    for i in range(30):\n",
    "        model.fit(x_train, y_train, nb_epoch = 2, batch_size = int(x_train.shape[0]), \n",
    "                  verbose = 2, class_weight={0:base_weights[0],1:base_weights[1]}, \n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        y_train_pred = model.predict(x_train).argmax(axis=1)\n",
    "        y_test_pred = model.predict(x_test).argmax(axis=1)\n",
    "        f1train = metrics.f1_score(y_train, y_train_pred)\n",
    "        f1test = metrics.f1_score(y_test, y_test_pred)\n",
    "        f_train.append(f1train)\n",
    "        f.append(f1test)\n",
    "        print(i)\n",
    "        print(f1train, f1test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
