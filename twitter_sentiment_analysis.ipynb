{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:33.491678Z",
     "start_time": "2018-11-30T22:28:32.385260Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:33.550921Z",
     "start_time": "2018-11-30T22:28:33.493687Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_E6oV3lV.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NORMALIZATION AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:36.267116Z",
     "start_time": "2018-11-30T22:28:36.228013Z"
    }
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    #-- Remove HTML special entities (e.g. &amp;)\n",
    "    #tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    #-- Convert sdsds@username to EMAIL\n",
    "    tweet = re.sub('\\S+@\\S*','EMAIL',tweet)\n",
    "    #-- Convert @username to AT_USER\n",
    "    tweet = re.sub('\\b@\\S*','AT_USER',tweet)\n",
    "    #-- Remove tickers\n",
    "    #tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    #-- lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #-- Remove hyperlinks\n",
    "    tweet = re.sub(r'\\bhttps?:\\/\\/\\S*', '', tweet)\n",
    "    tweet = re.sub(r'\\bwww.*\\S*', '', tweet)\n",
    "    #-- Remove hashtags\n",
    "    tweet = re.sub(r'#', ' ', tweet)\n",
    "    #-- remove punctuation and special characters\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9]', ' ', tweet)\n",
    "    #-- Remove words with 2 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    #-- Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    #-- Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ') \n",
    "    #-- Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    #tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:37.313972Z",
     "start_time": "2018-11-30T22:28:37.280377Z"
    }
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    #-- Convert sdsds@username to _email\n",
    "    tweet = re.sub('\\S+@\\S*','_email',tweet)\n",
    "    #-- Convert @username to _user\n",
    "    tweet = re.sub('\\b@\\S*','_user',tweet)\n",
    "    #-- Convert number to _number\n",
    "    tweet = re.sub(r'\\b[0-9]+\\b', '_number', tweet)\n",
    "    #-- lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #-- Remove hyperlinks\n",
    "    tweet = re.sub(r'\\bhttps?:\\/\\/\\S*', '', tweet)\n",
    "    tweet = re.sub(r'\\bwww.*\\S*', '', tweet)\n",
    "    #-- Convert hashtags into _hashtag\n",
    "    tweet = re.sub(r'#', '  _hashtag ', tweet)\n",
    "    #-- remove punctuation and special characters and digits\n",
    "    tweet = re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "    #-- Remove words with 2 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    #-- Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    #-- Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ') \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:43.001855Z",
     "start_time": "2018-11-30T22:28:41.891683Z"
    }
   },
   "outputs": [],
   "source": [
    "df['tweet_processed'] = df['tweet'].apply(processTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:38.848158Z",
     "start_time": "2018-11-30T22:29:28.639268Z"
    }
   },
   "outputs": [],
   "source": [
    "df['tweet_processed'] = df['tweet_processed'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T15:19:18.585251Z",
     "start_time": "2018-12-01T15:19:18.347571Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/tweetsbytextblob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:38.911989Z",
     "start_time": "2018-12-01T14:31:38.889929Z"
    }
   },
   "outputs": [],
   "source": [
    "#stop_words = stopwords.words('english')#the filtering will be done when using CountVectorizer in 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:39.690527Z",
     "start_time": "2018-12-01T14:31:38.913994Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['tweet_processed'] = df['tweet_processed'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:42.917327Z",
     "start_time": "2018-12-01T14:31:39.692875Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "df['tweet_processed'] = df['tweet_processed'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:42.925349Z",
     "start_time": "2018-12-01T14:31:42.918333Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create class to incorporate lemmatization \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "#lemmatizationn will occur when using CountVectorizer in 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:31:42.933370Z",
     "start_time": "2018-12-01T14:31:42.927354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:08.839929Z",
     "start_time": "2018-11-30T22:16:08.350523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other_stop = [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would']\n",
    "#create array with the number of each word occurence for each tweet\n",
    "#cv = CountVectorizer(max_df=0.9, min_df=15, tokenizer=LemmaTokenizer(), stop_words=stop_words + other_stop)\n",
    "cv = CountVectorizer(max_df=0.9, min_df=15)\n",
    "word_count_vect = cv.fit_transform(df.tweet_processed.tolist())\n",
    "word_count_vect.toarray()#one array per document with the number of apperanced of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:11.755205Z",
     "start_time": "2018-11-30T22:16:11.751193Z"
    }
   },
   "outputs": [],
   "source": [
    "#map from words to index\n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:12.276722Z",
     "start_time": "2018-11-30T22:16:12.272657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2153"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:22.253994Z",
     "start_time": "2018-11-30T22:16:22.218896Z"
    }
   },
   "outputs": [],
   "source": [
    "#compute tfidf for each tweet\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vect)\n",
    "tf_idf_vector = tfidf_transformer.transform(word_count_vect)\n",
    "#tf_idf_vector.toarray()#one array per document with the tfidf each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:24.079985Z",
     "start_time": "2018-11-30T22:16:23.944090Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tf_idf_vector.toarray()\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:26.134724Z",
     "start_time": "2018-11-30T22:16:25.544663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:26.140768Z",
     "start_time": "2018-11-30T22:16:26.136731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'label', 'tweet', 'tweet_processed'], dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:27.445948Z",
     "start_time": "2018-11-30T22:16:27.297476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = []\n",
    "for i,t in zip(df.index,df.tweet_processed):\n",
    "    labeled_tweets.append(LabeledSentence(t.split(), [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:16:28.558655Z",
     "start_time": "2018-11-30T22:16:28.550130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppreto\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0,size=100,negative=5,min_count=2,alpha=0.065,min_alpha=0.065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:31.999469Z",
     "start_time": "2018-11-30T22:16:29.034502Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dbow.build_vocab([x for x in labeled_tweets])\n",
    "for epoch in range(60):\n",
    "    model_dbow.train(utils.shuffle([x for x in labeled_tweets]), total_examples=len(labeled_tweets), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.155836Z",
     "start_time": "2018-11-30T22:17:32.002476Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2_vecs = np.zeros((df.shape[0], 100))\n",
    "j = 0\n",
    "for i in df.index:\n",
    "    prefix = str(i)\n",
    "    doc2_vecs[i] = model_dbow.docvecs[i]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.172880Z",
     "start_time": "2018-11-30T22:17:32.157855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.67746383, -0.85131454, -0.60374868, ..., -0.7659356 ,\n",
       "       -0.793423  , -0.47833678])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2_vecs.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.913018Z",
     "start_time": "2018-11-30T22:17:32.175894Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([tf_idf_vector.toarray(), doc2_vecs], axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.961989Z",
     "start_time": "2018-11-30T22:17:32.914020Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve, classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.992035Z",
     "start_time": "2018-11-30T22:17:32.963960Z"
    }
   },
   "outputs": [],
   "source": [
    "base_weights = 1/(df.label.value_counts()/max(df.label.value_counts())).values\n",
    "#base_weights[1] = base_weights[1]*1.4\n",
    "weights = np.array([base_weights[0]  if x == 0 else base_weights[1] for x in df.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:17:32.998052Z",
     "start_time": "2018-11-30T22:17:32.994040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 2253)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T22:28:24.351915Z",
     "start_time": "2018-11-30T22:17:33.004067Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "creating model\n",
      "training model\n",
      "predicting\n",
      "Fold:  1\n",
      "Train report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23776\n",
      "           1       0.97      1.00      0.99      1793\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     25569\n",
      "   macro avg       0.99      1.00      0.99     25569\n",
      "weighted avg       1.00      1.00      1.00     25569\n",
      "\n",
      "------------\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      5944\n",
      "           1       0.63      0.58      0.60       449\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      6393\n",
      "   macro avg       0.80      0.78      0.79      6393\n",
      "weighted avg       0.94      0.95      0.95      6393\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAIDCAYAAABfHonuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e70007cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "creating model\n",
      "training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-da553a593811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m                   \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logloss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                   \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                   \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                   )\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1045\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create object for cross validation (5 folds)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "fold = 1\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "print(\"training\")\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    weights_train = weights[train_index]\n",
    "    \n",
    "    \"\"\"\n",
    "    #sample data\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    d = dict(zip(unique, counts))\n",
    "    # undersample data points\n",
    "    \n",
    "    undersampler = RandomUnderSampler(sampling_strategy={1: d[1],\n",
    "                                                         2: d[2],\n",
    "                                                         3: int(round(d[3] / 2))},\n",
    "                                      random_state=random_state)\n",
    "    \n",
    "    oversampler = SMOTE(sampling_strategy={0: d[0],\n",
    "                                           1: d[0],\n",
    "                                           },\n",
    "                        random_state=20)\n",
    "    \n",
    "    #X_resampled, y_resampled = undersampler.fit_resample(X_train.fillna(-1), y_train)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    #X_resampled[X_resampled==-1] = np.NaN\n",
    "    \n",
    "    #X_resampled = pd.DataFrame(X_resampled, columns=X_train.columns.values)\n",
    "    \"\"\"\n",
    "    # train model\n",
    "    print(\"creating model\")\n",
    "    xgb_model = XGBClassifier(objective='binary:logistic',\n",
    "                              n_estimators=300,\n",
    "                              max_depth=10,\n",
    "                              learning_rate=0.05,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=20)\n",
    "    print(\"training model\")\n",
    "    xgb_model.fit(X_train, \n",
    "                  y_train,\n",
    "                  sample_weight=weights_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  early_stopping_rounds=30,\n",
    "                  verbose=False\n",
    "                  )\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    # predict outputs\n",
    "    train_predictions = xgb_model.predict(X_train)\n",
    "    train_actuals = y_train\n",
    "    \n",
    "    predictions = xgb_model.predict(X_test)\n",
    "    prediction_probs = xgb_model.predict_proba(X_test)\n",
    "    actuals = y_test\n",
    "    \n",
    "    # get train performance metrics\n",
    "    print(\"Fold: \", fold)\n",
    "    print('Train report')\n",
    "    print(classification_report(train_actuals, train_predictions)) \n",
    "    print('------------')\n",
    "    \n",
    "    # get performance metrics\n",
    "    print('Test report')\n",
    "    print(classification_report(actuals, predictions)) \n",
    "    precisions.append(precision_score(actuals, predictions, average=None))\n",
    "    recalls.append(recall_score(actuals, predictions, average=None))\n",
    "                 \n",
    "    # precision-recall curve\n",
    "    p_curve = dict()\n",
    "    r_curve = dict()\n",
    "    colors = ['navy', 'turquoise']\n",
    "    for j in range(2):\n",
    "        temp_actuals = np.copy(actuals)\n",
    "        temp_actuals[temp_actuals != j] = 0\n",
    "        temp_actuals[temp_actuals == j] = 1\n",
    "        p_curve[j], r_curve[j], _ = precision_recall_curve(temp_actuals, prediction_probs[:, j])\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for i, color in zip(range(2), colors):\n",
    "        l, = plt.plot(r_curve[i], p_curve[i], color=color, lw=2)\n",
    "        lines.append(l)\n",
    "        labels.append('Precision-recall for class {0}'.format(i))\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve for not racist (0), racist (1)')\n",
    "    plt.legend(lines, labels, prop=dict(size=14))\n",
    "    plt.grid()\n",
    "    #plt.savefig(os.path.join(OUT_DIR, 'images', 'experiment_4', 'p-r_curve_fold{f}.png'.format(f=fold)))\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T16:49:08.517210Z",
     "start_time": "2018-11-30T16:49:08.501630Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential #Sequential Models\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Dropout #Dense Fully Connected Layer Type\n",
    "from keras.optimizers import SGD #Stochastic Gradient Descent Optimizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T18:54:09.083689Z",
     "start_time": "2018-11-30T18:54:09.076696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 784)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T18:58:27.792910Z",
     "start_time": "2018-11-30T18:58:27.771854Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_network(n_columns, lr=0.001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_shape=(n_columns,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "              \n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "    #stochastic gradient descent\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T18:58:29.834512Z",
     "start_time": "2018-11-30T18:58:29.829498Z"
    }
   },
   "outputs": [],
   "source": [
    "base_weights = 1/(df.label.value_counts()/max(df.label.value_counts())).values\n",
    "#base_weights[1] = base_weights[1]*1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T19:06:04.850678Z",
     "start_time": "2018-11-30T18:58:38.247218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 297us/step - loss: 1.7911 - val_loss: 2.4566\n",
      "0.18470539072294187\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 1.9389 - val_loss: 0.5486\n",
      "0.36003861003861\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 206us/step - loss: 0.7930 - val_loss: 0.2035\n",
      "0.4963350785340314\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.7822 - val_loss: 0.2953\n",
      "0.4662402274342572\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.6225 - val_loss: 0.2720\n",
      "0.4816446402349487\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.5653 - val_loss: 0.2522\n",
      "0.4928193499622071\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.5140 - val_loss: 0.2265\n",
      "0.5181598062953995\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.4728 - val_loss: 0.2096\n",
      "0.525781910397295\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 210us/step - loss: 0.4389 - val_loss: 0.2014\n",
      "0.5242718446601942\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 204us/step - loss: 0.4089 - val_loss: 0.2001\n",
      "0.530396475770925\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 214us/step - loss: 0.3807 - val_loss: 0.2039\n",
      "0.5340136054421769\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 199us/step - loss: 0.3536 - val_loss: 0.2078\n",
      "0.5353535353535354\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 195us/step - loss: 0.3284 - val_loss: 0.2095\n",
      "0.5312239800166528\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 204us/step - loss: 0.3051 - val_loss: 0.2059\n",
      "0.5392491467576791\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.2835 - val_loss: 0.1982\n",
      "0.5489849955869374\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 196us/step - loss: 0.2628 - val_loss: 0.1881\n",
      "0.5562790697674419\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 193us/step - loss: 0.2430 - val_loss: 0.1784\n",
      "0.56640625\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.2241 - val_loss: 0.1704\n",
      "0.5740551583248212\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.2063 - val_loss: 0.1651\n",
      "0.5811965811965812\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 195us/step - loss: 0.1896 - val_loss: 0.1619\n",
      "0.588495575221239\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 197us/step - loss: 0.1741 - val_loss: 0.1603\n",
      "0.5963718820861678\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 198us/step - loss: 0.1598 - val_loss: 0.1594\n",
      "0.5931034482758621\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.1464 - val_loss: 0.1592\n",
      "0.6000000000000001\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.1341 - val_loss: 0.1591\n",
      "0.6072684642438453\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.1227 - val_loss: 0.1593\n",
      "0.6137689614935823\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.1122 - val_loss: 0.1596\n",
      "0.6223776223776224\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.1025 - val_loss: 0.1602\n",
      "0.6248548199767712\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0938 - val_loss: 0.1609\n",
      "0.6242774566473988\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0858 - val_loss: 0.1619\n",
      "0.6265895953757226\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0785 - val_loss: 0.1630\n",
      "0.621559633027523\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0719 - val_loss: 0.1642\n",
      "0.6214039125431532\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0658 - val_loss: 0.1655\n",
      "0.6171428571428571\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0603 - val_loss: 0.1669\n",
      "0.6197502837684449\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0552 - val_loss: 0.1683\n",
      "0.6162528216704288\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0506 - val_loss: 0.1698\n",
      "0.6164229471316085\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.0464 - val_loss: 0.1712\n",
      "0.6122905027932961\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 193us/step - loss: 0.0426 - val_loss: 0.1727\n",
      "0.6112956810631228\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 192us/step - loss: 0.0391 - val_loss: 0.1743\n",
      "0.6106194690265487\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 198us/step - loss: 0.0359 - val_loss: 0.1759\n",
      "0.6108048511576627\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 231us/step - loss: 0.0330 - val_loss: 0.1776\n",
      "0.6086956521739131\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.0303 - val_loss: 0.1795\n",
      "0.6040992448759439\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 191us/step - loss: 0.0279 - val_loss: 0.1813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-b78d95192a66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                   validation_data=(x_test, y_test))\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mf1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1025\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[1;32m-> 1027\u001b[1;33m                                   steps=steps)\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1798\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1800\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1299\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "f = []\n",
    "#start cross validation\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "    #create model\n",
    "    model = create_network(x_train.shape[1], 0.05)\n",
    "    for i in range(100):\n",
    "        model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n",
    "                      epochs=1, verbose=1, class_weight={0:base_weights[0],1:base_weights[1]}, \n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        y_train_pred = model.predict(x_train).argmax(axis=1)\n",
    "        y_test_pred = model.predict(x_test).argmax(axis=1)\n",
    "        f1s = metrics.f1_score(y_test, y_test_pred)\n",
    "        f.append(f1s)\n",
    "        print(f1s)\n",
    "        #print(\"train\\n\",metrics.classification_report(y_train, y_train_pred))\n",
    "        #print(\"test\\n\",metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T19:07:36.022674Z",
     "start_time": "2018-11-30T19:07:35.980520Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_network(n_columns, lr=0.001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_shape=(n_columns,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "              \n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    #stochastic gradient descent\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T20:36:20.491331Z",
     "start_time": "2018-11-30T19:11:59.023557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 311us/step - loss: 1.5358 - val_loss: 0.8981\n",
      "0.21091997008227376\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 183us/step - loss: 0.8940 - val_loss: 0.4814\n",
      "0.3337868480725624\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 184us/step - loss: 0.7222 - val_loss: 0.4165\n",
      "0.3738317757009346\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 189us/step - loss: 0.6603 - val_loss: 0.3833\n",
      "0.4011299435028248\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.6212 - val_loss: 0.3484\n",
      "0.4297820823244553\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.5882 - val_loss: 0.3115\n",
      "0.4441575209812783\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 184us/step - loss: 0.5555 - val_loss: 0.2750\n",
      "0.47491166077738517\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 185us/step - loss: 0.5222 - val_loss: 0.2432\n",
      "0.5022970903522205\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 11s 413us/step - loss: 0.4894 - val_loss: 0.2179\n",
      "0.529074529074529\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 292us/step - loss: 0.4581 - val_loss: 0.1992\n",
      "0.5512367491166078\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 250us/step - loss: 0.4294 - val_loss: 0.1864\n",
      "0.5583566760037348\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 262us/step - loss: 0.4034 - val_loss: 0.1779\n",
      "0.5700483091787439\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.3799 - val_loss: 0.1726\n",
      "0.5801376597836776\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.3582 - val_loss: 0.1695\n",
      "0.5810945273631841\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 269us/step - loss: 0.3378 - val_loss: 0.1681\n",
      "0.5868263473053891\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 253us/step - loss: 0.3181 - val_loss: 0.1678\n",
      "0.586\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.2990 - val_loss: 0.1677\n",
      "0.5890547263681591\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.2805 - val_loss: 0.1672\n",
      "0.5902293120638086\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.2626 - val_loss: 0.1660\n",
      "0.5965965965965967\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.2455 - val_loss: 0.1641\n",
      "0.5925176946410515\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.2292 - val_loss: 0.1614\n",
      "0.6018423746161718\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 269us/step - loss: 0.2138 - val_loss: 0.1586\n",
      "0.6075156576200418\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 273us/step - loss: 0.1993 - val_loss: 0.1560\n",
      "0.6102783725910065\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.1856 - val_loss: 0.1539\n",
      "0.6111719605695509\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.1729 - val_loss: 0.1524\n",
      "0.6140155728587319\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 257us/step - loss: 0.1609 - val_loss: 0.1514\n",
      "0.6133032694475762\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.1497 - val_loss: 0.1509\n",
      "0.6068181818181818\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.1392 - val_loss: 0.1506\n",
      "0.611683848797251\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.1295 - val_loss: 0.1505\n",
      "0.6102857142857143\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.1204 - val_loss: 0.1507\n",
      "0.6111744583808437\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.1119 - val_loss: 0.1510\n",
      "0.6136363636363636\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.1041 - val_loss: 0.1514\n",
      "0.6092865232163079\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.0969 - val_loss: 0.1520\n",
      "0.6098654708520179\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 271us/step - loss: 0.0902 - val_loss: 0.1527\n",
      "0.6119733924611973\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 258us/step - loss: 0.0839 - val_loss: 0.1536\n",
      "0.614709110867179\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0782 - val_loss: 0.1546\n",
      "0.6132177681473456\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 256us/step - loss: 0.0728 - val_loss: 0.1556\n",
      "0.6167023554603854\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0679 - val_loss: 0.1567\n",
      "0.6178343949044586\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 247us/step - loss: 0.0633 - val_loss: 0.1579\n",
      "0.6158730158730158\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 259us/step - loss: 0.0591 - val_loss: 0.1591\n",
      "0.6140904311251315\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0551 - val_loss: 0.1603\n",
      "0.6157068062827225\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 268us/step - loss: 0.0515 - val_loss: 0.1615\n",
      "0.6153846153846154\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 264us/step - loss: 0.0481 - val_loss: 0.1628\n",
      "0.6149068322981367\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.0450 - val_loss: 0.1640\n",
      "0.6127049180327869\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0421 - val_loss: 0.1653\n",
      "0.6095820591233436\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 267us/step - loss: 0.0395 - val_loss: 0.1666\n",
      "0.6085192697768763\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 266us/step - loss: 0.0370 - val_loss: 0.1680\n",
      "0.6066734074823055\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 265us/step - loss: 0.0347 - val_loss: 0.1694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6088709677419355\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 256us/step - loss: 0.0326 - val_loss: 0.1708\n",
      "0.6132264529058116\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 225us/step - loss: 0.0306 - val_loss: 0.1722\n",
      "0.6127744510978045\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 3930s 154ms/step - loss: 0.0288 - val_loss: 0.1736\n",
      "0.6123260437375746\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 8s 298us/step - loss: 0.0271 - val_loss: 0.1750\n",
      "0.6106719367588933\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 194us/step - loss: 0.0256 - val_loss: 0.1762\n",
      "0.6094674556213018\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 5s 190us/step - loss: 0.0241 - val_loss: 0.1775\n",
      "0.6070726915520629\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 227us/step - loss: 0.0228 - val_loss: 0.1788\n",
      "0.603515625\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 221us/step - loss: 0.0215 - val_loss: 0.1800\n",
      "0.6064139941690962\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 243us/step - loss: 0.0203 - val_loss: 0.1811\n",
      "0.6077669902912622\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 254us/step - loss: 0.0193 - val_loss: 0.1823\n",
      "0.6060019361084221\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 261us/step - loss: 0.0183 - val_loss: 0.1833\n",
      "0.6060019361084221\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 254us/step - loss: 0.0173 - val_loss: 0.1843\n",
      "0.6051873198847262\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 248us/step - loss: 0.0165 - val_loss: 0.1852\n",
      "0.6063522617901829\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.0157 - val_loss: 0.1862\n",
      "0.6050096339113681\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 244us/step - loss: 0.0149 - val_loss: 0.1870\n",
      "0.6038461538461538\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 251us/step - loss: 0.0142 - val_loss: 0.1878\n",
      "0.6051873198847262\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 242us/step - loss: 0.0136 - val_loss: 0.1885\n",
      "0.6057692307692308\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 257us/step - loss: 0.0130 - val_loss: 0.1891\n",
      "0.6055930568948891\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 250us/step - loss: 0.0124 - val_loss: 0.1897\n",
      "0.6044273339749759\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 248us/step - loss: 0.0119 - val_loss: 0.1902\n",
      "0.6057692307692308\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 6s 252us/step - loss: 0.0114 - val_loss: 0.1906\n",
      "0.6065259117082534\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n",
      "25569/25569 [==============================] - 7s 255us/step - loss: 0.0110 - val_loss: 0.1911\n",
      "0.6040268456375838\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-4b07cf8460f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n\u001b[0;32m     13\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbase_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbase_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                   validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "f = []\n",
    "#start cross validation\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "    #create model\n",
    "    model = create_network(x_train.shape[1], 0.05)\n",
    "    for i in range(100):\n",
    "        model.fit(x_train, y_train, batch_size=int(x_train.shape[0]), \n",
    "                      epochs=1, verbose=1, class_weight={0:base_weights[0],1:base_weights[1]}, \n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        y_train_pred = np.around(model.predict(x_train))\n",
    "        y_test_pred = np.around(model.predict(x_test))\n",
    "        f1s = metrics.f1_score(y_test, y_test_pred)\n",
    "        f.append(f1s)\n",
    "        print(f1s)\n",
    "        #print(\"train\\n\",metrics.classification_report(y_train, y_train_pred))\n",
    "        #print(\"test\\n\",metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T19:11:14.189122Z",
     "start_time": "2018-11-30T19:11:13.055413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
